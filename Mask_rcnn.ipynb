{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur0TB_vlao_a"
      },
      "outputs": [],
      "source": [
        "!pip install Pillow numpy torch torchvision onnx onnxruntime==1.12.0 matplotlib opencv-python pycocotools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zfIRV8INr2qY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yMIpMo7_Z2rn"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms, models\n",
        "from onnx import numpy_helper\n",
        "import os\n",
        "import onnxruntime as rt\n",
        "from matplotlib.colors import hsv_to_rgb\n",
        "import cv2\n",
        "# import gradio as gr\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "import pycocotools.mask as mask_util\n",
        "\n",
        "def preprocess(image):\n",
        "    # Resize\n",
        "    ratio = 800.0 / min(image.size[0], image.size[1])\n",
        "    image = image.resize((int(ratio * image.size[0]), int(ratio * image.size[1])), Image.BILINEAR)\n",
        "\n",
        "    # Convert to BGR\n",
        "    image = np.array(image)[:, :, [2, 1, 0]].astype('float32')\n",
        "\n",
        "    # HWC -> CHW\n",
        "    image = np.transpose(image, [2, 0, 1])\n",
        "\n",
        "    # Normalize\n",
        "    mean_vec = np.array([102.9801, 115.9465, 122.7717])\n",
        "    for i in range(image.shape[0]):\n",
        "        image[i, :, :] = image[i, :, :] - mean_vec[i]\n",
        "\n",
        "    # Pad to be divisible of 32\n",
        "    import math\n",
        "    padded_h = int(math.ceil(image.shape[1] / 32) * 32)\n",
        "    padded_w = int(math.ceil(image.shape[2] / 32) * 32)\n",
        "\n",
        "    padded_image = np.zeros((3, padded_h, padded_w), dtype=np.float32)\n",
        "    padded_image[:, :image.shape[1], :image.shape[2]] = image\n",
        "    image = padded_image\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "\n",
        "# Start from ORT 1.10, ORT requires explicitly setting the providers parameter if you want to use execution providers\n",
        "# other than the default CPU provider (as opposed to the previous behavior of providers getting set/registered by default\n",
        "# based on the build flags) when instantiating InferenceSession.\n",
        "# For example, if NVIDIA GPU is available and ORT Python package is built with CUDA, then call API as following:\n",
        "# onnxruntime.InferenceSession(path/to/model, providers=['CUDAExecutionProvider'])\n",
        "os.system(\"wget https://github.com/AK391/models/raw/main/vision/object_detection_segmentation/mask-rcnn/model/MaskRCNN-10.onnx\")\n",
        "sess = rt.InferenceSession(\"MaskRCNN-10.onnx\")\n",
        "\n",
        "outputs = sess.get_outputs()\n",
        "\n",
        "\n",
        "classes = [\n",
        "    \"__background\",\n",
        "    \"person\",\n",
        "    \"bicycle\",\n",
        "    \"car\",\n",
        "    \"motorcycle\",\n",
        "    \"airplane\",\n",
        "    \"bus\",\n",
        "    \"train\",\n",
        "    \"truck\",\n",
        "    \"boat\",\n",
        "    \"traffic light\",\n",
        "    \"fire hydrant\",\n",
        "    \"stop sign\",\n",
        "    \"parking meter\",\n",
        "    \"bench\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"dog\",\n",
        "    \"horse\",\n",
        "    \"sheep\",\n",
        "    \"cow\",\n",
        "    \"elephant\",\n",
        "    \"bear\",\n",
        "    \"zebra\",\n",
        "    \"giraffe\",\n",
        "    \"backpack\",\n",
        "    \"umbrella\",\n",
        "    \"handbag\",\n",
        "    \"tie\",\n",
        "    \"suitcase\",\n",
        "    \"frisbee\",\n",
        "    \"skis\",\n",
        "    \"snowboard\",\n",
        "    \"sports ball\",\n",
        "    \"kite\",\n",
        "    \"baseball bat\",\n",
        "    \"baseball glove\",\n",
        "    \"skateboard\",\n",
        "    \"surfboard\",\n",
        "    \"tennis racket\",\n",
        "    \"bottle\",\n",
        "    \"wine glass\",\n",
        "    \"cup\",\n",
        "    \"fork\",\n",
        "    \"knife\",\n",
        "    \"spoon\",\n",
        "    \"bowl\",\n",
        "    \"banana\",\n",
        "    \"apple\",\n",
        "    \"sandwich\",\n",
        "    \"orange\",\n",
        "    \"broccoli\",\n",
        "    \"carrot\",\n",
        "    \"hot dog\",\n",
        "    \"pizza\",\n",
        "    \"donut\",\n",
        "    \"cake\",\n",
        "    \"chair\",\n",
        "    \"couch\",\n",
        "    \"potted plant\",\n",
        "    \"bed\",\n",
        "    \"dining table\",\n",
        "    \"toilet\",\n",
        "    \"tv\",\n",
        "    \"laptop\",\n",
        "    \"mouse\",\n",
        "    \"remote\",\n",
        "    \"keyboard\",\n",
        "    \"cell phone\",\n",
        "    \"microwave\",\n",
        "    \"oven\",\n",
        "    \"toaster\",\n",
        "    \"sink\",\n",
        "    \"refrigerator\",\n",
        "    \"book\",\n",
        "    \"clock\",\n",
        "    \"vase\",\n",
        "    \"scissors\",\n",
        "    \"teddy bear\",\n",
        "    \"hair drier\",\n",
        "    \"toothbrush\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "def display_objdetect_image(image, boxes, labels, scores, masks, score_threshold=0.7):\n",
        "    # Resize boxes\n",
        "    ratio = 800.0 / min(image.size[0], image.size[1])\n",
        "    boxes /= ratio\n",
        "\n",
        "    _, ax = plt.subplots(1, figsize=(12,9))\n",
        "\n",
        "    image = np.array(image)\n",
        "\n",
        "    for mask, box, label, score in zip(masks, boxes, labels, scores):\n",
        "        # Showing boxes with score > 0.7\n",
        "        if score <= score_threshold:\n",
        "            continue\n",
        "\n",
        "        # Finding contour based on mask\n",
        "        mask = mask[0, :, :, None]\n",
        "        int_box = [int(i) for i in box]\n",
        "        mask = cv2.resize(mask, (int_box[2]-int_box[0]+1, int_box[3]-int_box[1]+1))\n",
        "        mask = mask > 0.5\n",
        "        im_mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
        "        x_0 = max(int_box[0], 0)\n",
        "        x_1 = min(int_box[2] + 1, image.shape[1])\n",
        "        y_0 = max(int_box[1], 0)\n",
        "        y_1 = min(int_box[3] + 1, image.shape[0])\n",
        "        mask_y_0 = max(y_0 - box[1], 0)\n",
        "        mask_y_1 = mask_y_0 + y_1 - y_0\n",
        "        mask_x_0 = max(x_0 - box[0], 0)\n",
        "        mask_x_1 = mask_x_0 + x_1 - x_0\n",
        "        im_mask[y_0:y_1, x_0:x_1] = mask[\n",
        "            mask_y_0 : mask_y_1, mask_x_0 : mask_x_1\n",
        "        ]\n",
        "        im_mask = im_mask[:, :, None]\n",
        "\n",
        "        # OpenCV version 4.x\n",
        "        contours, hierarchy = cv2.findContours(\n",
        "            im_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        image = cv2.drawContours(image, contours, -1, 25, 3)\n",
        "\n",
        "        rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='b', facecolor='none')\n",
        "        ax.annotate(classes[label] + ':' + str(np.round(score, 2)), (box[0], box[1]), color='w', fontsize=12)\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    # ax.imshow(image)\n",
        "    # plt.axis('off')\n",
        "    # plt.savefig('out.png', bbox_inches='tight')\n",
        "\n",
        "\n",
        "def inference(img):\n",
        "  input_image = Image.open(img)\n",
        "  orig_tensor = np.asarray(input_image)\n",
        "  input_tensor = preprocess(input_image)\n",
        "\n",
        "  output_names = list(map(lambda output: output.name, outputs))\n",
        "  input_name = sess.get_inputs()[0].name\n",
        "\n",
        "  boxes, labels, scores, masks = sess.run(output_names, {input_name: input_tensor})\n",
        "  # display_objdetect_image(input_image, boxes, labels, scores, masks)\n",
        "  return 'out.png'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DghJGhdcdw6",
        "outputId": "1d8baf03-be6c-499f-b991-17858203cbea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1119.png processed\n",
            "1139.png processed\n",
            "1342.png processed\n",
            "1138.png processed\n",
            "1154.png processed\n",
            "1160.png processed\n",
            "1043.png processed\n",
            "118.png processed\n",
            "1111.png processed\n",
            "1055.png processed\n",
            "1133.png processed\n",
            "831.png processed\n",
            "1021.png processed\n",
            "1095.png processed\n",
            "124.png processed\n",
            "1171.png processed\n",
            "127.png processed\n",
            "1323.png processed\n",
            "1015.png processed\n",
            "1054.png processed\n",
            "1187.png processed\n",
            "921.png processed\n",
            "1065.png processed\n",
            "1023.png processed\n",
            "1269.png processed\n",
            "126.png processed\n",
            "1382.png processed\n",
            "1345.png processed\n",
            "1122.png processed\n",
            "1101.png processed\n",
            "1113.png processed\n",
            "1034.png processed\n",
            "1167.png processed\n",
            "133.png processed\n",
            "1302.png processed\n",
            "1295.png processed\n",
            "1145.png processed\n",
            "1058.png processed\n",
            "1280.png processed\n",
            "1311.png processed\n",
            "1206.png processed\n",
            "1245.png processed\n",
            "1337.png processed\n",
            "109.png processed\n",
            "1310.png processed\n",
            "1256.png processed\n",
            "888.png processed\n",
            "1174.png processed\n",
            "1352.png processed\n",
            "1026.png processed\n",
            "1320.png processed\n",
            "1339.png processed\n",
            "116.png processed\n",
            "1287.png processed\n",
            "1125.png processed\n",
            "1069.png processed\n",
            "1045.png processed\n",
            "1186.png processed\n",
            "1028.png processed\n",
            "954.png processed\n",
            "1239.png processed\n",
            "1093.png processed\n",
            "1089.png processed\n",
            "1123.png processed\n",
            "1300.png processed\n",
            "1232.png processed\n",
            "1209.png processed\n",
            "1357.png processed\n",
            "999.png processed\n",
            "1315.png processed\n",
            "923.png processed\n",
            "out.png processed\n",
            "1158.png processed\n",
            "1344.png processed\n",
            "1251.png processed\n",
            "998.png processed\n",
            "1048.png processed\n",
            "1047.png processed\n",
            "1029.png processed\n",
            "1116.png processed\n",
            "1137.png processed\n",
            "1027.png processed\n",
            "1253.png processed\n",
            "1052.png processed\n",
            "1195.png processed\n",
            "908.png processed\n",
            "1014.png processed\n",
            "1079.png processed\n",
            "1278.png processed\n",
            "925.png processed\n",
            "1104.png processed\n",
            "1281.png processed\n",
            "1176.png processed\n",
            "1003.png processed\n",
            "1338.png processed\n",
            "12.png processed\n",
            "1333.png processed\n",
            "1207.png processed\n",
            "1071.png processed\n",
            "1081.png processed\n",
            "1020.png processed\n",
            "1032.png processed\n",
            "1217.png processed\n",
            "1329.png processed\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "directory_path = \"/content/\"\n",
        "\n",
        "files = os.listdir(directory_path)\n",
        "results = []\n",
        "strt = time.time()\n",
        "count = 0\n",
        "for file in files:\n",
        "  if file.endswith(\".png\"):\n",
        "    count += 1\n",
        "    print(f\"{file} processed\")\n",
        "    result = inference(file)\n",
        "    results.append(result)\n",
        "end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Mh--XGFIctdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f5a232-ab23-4918-e8c9-4f0de72b6d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time of execution: 790.803416967392 seconds\n",
            "Average time per image: 7.603879009301846 seconds\n"
          ]
        }
      ],
      "source": [
        "# prompt: Calulate total time of execution in sec and average time\n",
        "\n",
        "total_time = end - strt\n",
        "average_time = total_time / count\n",
        "\n",
        "print(\"Total time of execution:\", total_time, \"seconds\")\n",
        "print(\"Average time per image:\", average_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "528db7Z5b9NZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}